{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717ce2fe-eccd-4f19-b4d9-9795e8faaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82858e29-2f9e-443d-8cda-7353d5ec9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65fc958a-b419-4ff4-9ead-a8c7b7f3a405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                                text  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"news_summary.csv\", encoding=\"latin1\", encoding_errors=\"ignore\")\n",
    "df = df[['text','ctext']].dropna()\n",
    "df.columns = ['summary', 'text']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371011c3-e61d-49dd-84fa-3f2ebffe55fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4396, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38eef1aa-90f4-4652-af83-6616cd73b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].values\n",
    "summaries = df['summary'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "839294bb-c506-41e0-a44a-96c33e721dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been decided to celebrate the festival of Rakshabandhan on August 7. In this connection, all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues,? the order, issued on August 1 by Gurpreet Singh, deputy secretary (personnel), had said.To ensure that no one skipped office, an attendance report was to be sent to the government the next evening.The two notifications ? one mandating the celebration of Rakshabandhan (left) and the other withdrawing the mandate (right) ? were issued by the Daman and Diu administration a day apart. The circular was withdrawn through a one-line order issued late in the evening by the UT?s department of personnel and administrative reforms.?The circular is ridiculous. There are sensitivities involved. How can the government dictate who I should tie rakhi to? We should maintain the professionalism of a workplace? an official told Hindustan Times earlier in the day. She refused to be identified.The notice was issued on Daman and Diu administrator and former Gujarat home minister Praful Kodabhai Patel?s direction, sources said.Rakshabandhan, a celebration of the bond between brothers and sisters, is one of several Hindu festivities and rituals that are no longer confined of private, family affairs but have become tools to push politic al ideologies.In 2014, the year BJP stormed to power at the Centre, Rashtriya Swayamsevak Sangh (RSS) chief Mohan Bhagwat said the festival had ?national significance? and should be celebrated widely ?to protect Hindu culture and live by the values enshrined in it?. The RSS is the ideological parent of the ruling BJP.Last year, women ministers in the Modi government went to the border areas to celebrate the festival with soldiers. A year before, all cabinet ministers were asked to go to their constituencies for the festival.\n",
      "The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(summaries[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2984da1f-4d51-46e4-8a3e-cc0461979ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 8000\n",
    "max_text_len = 100\n",
    "max_summary_len = 20\n",
    "\n",
    "tokenizer_text = Tokenizer(num_words=num_words)\n",
    "tokenizer_text.fit_on_texts(texts)\n",
    "\n",
    "tokenizer_sum = Tokenizer(num_words=num_words)\n",
    "tokenizer_sum.fit_on_texts(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a053fd5-e2a4-422f-8943-a8ed9f3bc7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer_text, open(\"tokenizer_text.pkl\", \"wb\"))\n",
    "pickle.dump(tokenizer_sum, open(\"tokenizer_sum.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6c23db8-6bea-4958-af1f-5c6710d66ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sequence shape: (4396, 100)\n",
      "Summary sequence shape: (4396, 20)\n"
     ]
    }
   ],
   "source": [
    "# Convert texts and summaries to sequences\n",
    "text_seq = tokenizer_text.texts_to_sequences(texts)\n",
    "summary_seq = tokenizer_sum.texts_to_sequences(summaries)\n",
    "\n",
    "# Padding sequences to fixed length\n",
    "text_seq = pad_sequences(text_seq, maxlen=max_text_len, padding='post')\n",
    "summary_seq = pad_sequences(summary_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "print(\"Text sequence shape:\", text_seq.shape)\n",
    "print(\"Summary sequence shape:\", summary_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84258956-af3e-4f6a-9257-0c68104b91ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024,000</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024,000</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]        │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]        │                 │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],              │\n",
       "│                               │                           │                 │ attention_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,104,000</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m1,024,000\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │       \u001b[38;5;34m1,024,000\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                 │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, │         \u001b[38;5;34m394,240\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│                               │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]        │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                 │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,  │         \u001b[38;5;34m394,240\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
       "│                               │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]        │                 │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_1 (\u001b[38;5;33mAttention\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],              │\n",
       "│                               │                           │                 │ attention_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m8000\u001b[0m)          │       \u001b[38;5;34m4,104,000\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,940,480</span> (26.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,940,480\u001b[0m (26.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,940,480</span> (26.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,940,480\u001b[0m (26.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "latent_dim = 256  # LSTM hidden size\n",
    "\n",
    "# -----------------------\n",
    "# 1. ENCODER\n",
    "# -----------------------\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "enc_emb = Embedding(num_words, 128)(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# -----------------------\n",
    "# 2. DECODER\n",
    "# -----------------------\n",
    "decoder_inputs = Input(shape=(max_summary_len,))\n",
    "dec_emb = Embedding(num_words, 128)(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# -----------------------\n",
    "# 3. ATTENTION\n",
    "# -----------------------\n",
    "attn_layer = Attention()\n",
    "attn_output = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# FIXED: Use Keras Concatenate instead of tf.concat\n",
    "decoder_concat = Concatenate(axis=-1)([decoder_outputs, attn_output])\n",
    "\n",
    "# -----------------------\n",
    "# 4. FINAL OUTPUT LAYER\n",
    "# -----------------------\n",
    "dense = Dense(num_words, activation='softmax')\n",
    "decoder_outputs_final = dense(decoder_concat)\n",
    "\n",
    "# -----------------------\n",
    "# 5. MODEL\n",
    "# -----------------------\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs_final)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "696dd989-7833-4b66-9ff6-4865f8b0396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder target shape: (4396, 20, 1)\n",
      "Epoch 1/3\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - loss: 7.2903 - val_loss: 6.5552\n",
      "Epoch 2/3\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - loss: 6.0896 - val_loss: 5.6644\n",
      "Epoch 3/3\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - loss: 5.0757 - val_loss: 4.6781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Decoder target must be one step ahead\n",
    "decoder_target = np.expand_dims(summary_seq, -1)\n",
    "\n",
    "print(\"Decoder target shape:\", decoder_target.shape)\n",
    "\n",
    "# -----------------------\n",
    "# TRAIN THE MODEL\n",
    "# -----------------------\n",
    "history = model.fit(\n",
    "    [text_seq, summary_seq],   # encoder input, decoder input\n",
    "    decoder_target,            # decoder output\n",
    "    epochs=3,                  # Increase to 10+ for better results\n",
    "    batch_size=64,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE MODEL AFTER TRAINING\n",
    "# -----------------------\n",
    "model.save(\"summarizer_model.h5\")\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74ac49a6-53eb-4415-b68c-60c590e43558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING: sample outputs\n",
      "-------------------------\n",
      "\n",
      "Original [0]:\n",
      "The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been...\n",
      "\n",
      "Summary:\n",
      "security there there there there there there there there there there president there president president president president president president president\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original [1]:\n",
      "From her special numbers to TV?appearances, Bollywood actor Malaika Arora Khan has managed to carve her own identity. The actor, who made her debut in the Hindi film industry with the blockbuster debut opposite Shah Rukh Khan in Chaiyya Chaiyya from Dil Se (1998), is still remembered for the song. However, for trolls, she is a woman first and what matters right now is that she divorced a ?rich man...\n",
      "\n",
      "Summary:\n",
      "there there there there there there there there there there there there president there president president president president president president\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original [2]:\n",
      "The Indira Gandhi Institute of Medical Sciences (IGIMS) in Patna amended its marital declaration form on Thursday, replacing the word ?virgin? with ?unmarried? after controversy.Until now, new recruits to the super-specialty medical institute in the state capital were required to declare if they were bachelors, widowers or virgins.IGIMS medical superintendent Dr Manish Mandal said institute direct...\n",
      "\n",
      "Summary:\n",
      "there there there there there there there there there there there president there president president president president president president president\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# CELL 9 — Inference function + tests\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load model & tokenizers (if you restarted kernel)\n",
    "# If model and tokenizers are already in memory you can skip re-loading\n",
    "try:\n",
    "    model  # if model present in memory\n",
    "except NameError:\n",
    "    model = tf.keras.models.load_model(\"summarizer_model.h5\", compile=False)\n",
    "\n",
    "try:\n",
    "    tokenizer_text\n",
    "    tokenizer_sum\n",
    "except NameError:\n",
    "    tokenizer_text = pickle.load(open(\"tokenizer_text.pkl\", \"rb\"))\n",
    "    tokenizer_sum  = pickle.load(open(\"tokenizer_sum.pkl\", \"rb\"))\n",
    "\n",
    "# Constants used during training\n",
    "max_text_len = 100\n",
    "max_summary_len = 20\n",
    "num_words = 8000  # same as training\n",
    "\n",
    "def decode_summary_from_seq(target_seq):\n",
    "    \"\"\"\n",
    "    Convert an array of token ids (1D) into a cleaned string using tokenizer_sum.index_word.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for t in target_seq:\n",
    "        t = int(t)\n",
    "        if t == 0:\n",
    "            continue\n",
    "        w = tokenizer_sum.index_word.get(t, \"\")\n",
    "        if not w:\n",
    "            continue\n",
    "        words.append(w)\n",
    "    # Join and cleanup spacing and special tokens (if any)\n",
    "    return \" \".join(words).strip()\n",
    "\n",
    "def summarize_text_greedy(text):\n",
    "    \"\"\"\n",
    "    Greedy decoding using the trained model: fills a target sequence step-by-step\n",
    "    by repeatedly predicting the next token and placing it into the target_seq.\n",
    "    This matches how we trained (teacher-forcing with full target sequences).\n",
    "    \"\"\"\n",
    "    # Preprocess input\n",
    "    seq = tokenizer_text.texts_to_sequences([text])\n",
    "    seq = pad_sequences(seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "    # Prepare empty target sequence (all zeros) and fill it progressively\n",
    "    target_seq = np.zeros((1, max_summary_len), dtype='int32')\n",
    "\n",
    "    for i in range(max_summary_len):\n",
    "        # model.predict returns a (1, max_summary_len, num_words) distribution.\n",
    "        preds = model.predict([seq, target_seq], verbose=0)\n",
    "        # pick the token for position i\n",
    "        token_i = np.argmax(preds[0, i])\n",
    "        target_seq[0, i] = token_i\n",
    "\n",
    "        # Optional early stop if token maps to empty or padding\n",
    "        # (depends on whether you used special tokens during training)\n",
    "        # if token_i == 0:\n",
    "        #     break\n",
    "\n",
    "    # Convert token ids to words\n",
    "    summary = decode_summary_from_seq(target_seq[0])\n",
    "    return summary\n",
    "\n",
    "# Quick tests: run for a few samples from your dataset (if available)\n",
    "print(\"TESTING: sample outputs\\n-------------------------\")\n",
    "for idx in range(3):\n",
    "    sample_text = texts[idx] if 'texts' in globals() else \"Your test article paragraph goes here.\"\n",
    "    print(f\"\\nOriginal [{idx}]:\\n{sample_text[:400]}...\\n\")\n",
    "    print(\"Summary:\")\n",
    "    print(summarize_text_greedy(sample_text))\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360de974-734a-416a-a1a7-7b2c8eb58d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
